# Airbnb High Booking Rate Prediction ğŸ¡ğŸ“ˆ

This repository contains an end-to-end machine learning pipeline for predicting **high booking rate Airbnb listings** (binary classification).  
The final solution achieved a **0.916 AUC on the hidden test set**, securing **2nd place** in the competition.

The project emphasizes clean experimentation, modular training pipelines, and reproducibility, with a strong focus on tree-based models and ensemble techniques.

---

## ğŸ† Results

- **Final Hidden Test AUC:** **0.916**
- **Leaderboard Position:** **2nd**
- **Primary Metric:** ROC-AUC

---

## ğŸ§  High-Level Approach

- Feature engineering and feature selection experiments
- K-fold cross-validation for robust model evaluation
- Extensive **XGBoost tuning** (manual + W&B sweeps)
- Experiments with imbalance handling, neural networks, and stacking
- Final retraining and submission generation from selected configurations

This README intentionally avoids documenting every experiment in detail; instead, it focuses on helping a new reader understand **where things live** and **how the pipeline fits together**.

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ config/                          # Centralized configuration files
â”‚   â”œâ”€â”€ model_config.yaml            # Core model & training configuration
â”‚   â”œâ”€â”€ sweep_config.yaml            # W&B hyperparameter sweep configuration
â”‚   â”œâ”€â”€ stacking_baseline_models.yaml# Baseline stacking model definitions
â”‚   â”œâ”€â”€ stacking_top5_models.yaml    # Top-5 stacking configuration
â”‚   â””â”€â”€ stacking_xgb_only.yaml       # XGBoost-only stacking setup
â”‚
â”œâ”€â”€ docs/                            # Project documentation / reports
â”œâ”€â”€ notebooks/                       # EDA and exploratory notebooks
â”œâ”€â”€ outputs/                         # Saved models, metrics, predictions, submissions
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ experiments/                 # Experimental & exploratory pipelines
â”‚   â”‚   â”œâ”€â”€ individual_models/       # Single-model baselines
â”‚   â”‚   â”œâ”€â”€ nn/                      # Neural network experiments
â”‚   â”‚   â”œâ”€â”€ smote/                   # Imbalance handling (SMOTE) experiments
â”‚   â”‚   â”œâ”€â”€ stacking/                # Stacking experiments and prototypes
â”‚   â”‚   â””â”€â”€ sweep_xgboost.py         # XGBoost hyperparameter sweeps (W&B)
â”‚   â”‚
â”‚   â”œâ”€â”€ feature_engineering/
â”‚   â”‚   â””â”€â”€ feature_selection_lasso_rfe.py
â”‚   â”‚                                  # Feature selection via LASSO and RFE
â”‚
â”‚   â”œâ”€â”€ model_training/              # Main training and inference pipeline
â”‚   â”‚   â”œâ”€â”€ model_defs.py             # Centralized model definitions
â”‚   â”‚   â”œâ”€â”€ run_kfold_training.py     # K-fold CV training entrypoint
â”‚   â”‚   â”œâ”€â”€ retrain_final_xgboost.py  # Retrain best model on full training data
â”‚   â”‚   â”œâ”€â”€ evaluate_test_set.py      # Evaluation on held-out data (if applicable)
â”‚   â”‚   â”œâ”€â”€ generate_submission.py    # Generate competition submission
â”‚   â”‚   â””â”€â”€ stacking_submission.py   # Submission pipeline for stacking models
â”‚   â”‚
â”‚   â””â”€â”€ utils/                       # Shared utilities (logging, IO, metrics)
â”‚
â”œâ”€â”€ wandb/                           # Weights & Biases run artifacts (local)
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md

````

---

## âš™ï¸ Configuration Files (`config/`)

All experiments and pipelines are driven by YAML configs:

- **`model_config.yaml`**  
  Core training configuration (model parameters, CV setup, paths, seeds).

- **`sweep_config.yaml`**  
  Weights & Biases sweep configuration for XGBoost hyperparameter tuning.

- **`stacking_baseline_models.yaml`**  
  Defines baseline models used in stacking experiments.

- **`stacking_top5_models.yaml`**  
  Stacking configuration using the top-performing individual models.

- **`stacking_xgb_only.yaml`**  
  Stacking setup restricted to XGBoost variants only.

---

## ğŸ› ï¸ Tech Stack

**Core Libraries**
- Python
- Pandas, NumPy
- scikit-learn
- XGBoost

**Experiment Tracking**
- **Weights & Biases (wandb)**  
  - Hyperparameter sweeps  
  - Metric tracking  
  - Run comparison and reproducibility  

**Other Tools**
- Matplotlib / Seaborn (EDA & diagnostics)
- YAML-based configuration management
- Modular logging and utilities

---

## â–¶ï¸ Typical Workflow

### 1ï¸âƒ£ Cross-Validation Training
Runs K-fold CV and logs metrics.

```bash
python src/model_training/run_kfold_training.py
````

### 2ï¸âƒ£ Retrain Final Model

Retrains the best-performing configuration on full training data.

```bash
python src/model_training/retrain_final_xgboost.py
```

### 3ï¸âƒ£ Generate Submission

Creates the final submission file.

```bash
python src/model_training/generate_submission.py
```

### (Optional) Stacking Submission

```bash
python src/model_training/stacking_submission.py
```

---

## ğŸ§ª Notes on Experiments

The `src/experiments/` directory contains exploratory work and alternative modeling strategies.
The **primary, production-style pipeline** lives in `src/model_training/`.

---

## ğŸ“Œ License

See `LICENSE`.


