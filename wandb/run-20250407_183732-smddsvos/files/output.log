
--- Training RandomForest ---

Fold 1
AUC: 0.8697
Classification Report:
              precision    recall  f1-score   support

           0       0.83      0.98      0.90     11158
           1       0.76      0.22      0.34      2836

    accuracy                           0.83     13994
   macro avg       0.80      0.60      0.62     13994
weighted avg       0.82      0.83      0.79     13994


Fold 2
AUC: 0.8748
Classification Report:
              precision    recall  f1-score   support

           0       0.84      0.98      0.90     11158
           1       0.77      0.24      0.37      2836

    accuracy                           0.83     13994
   macro avg       0.80      0.61      0.64     13994
weighted avg       0.82      0.83      0.79     13994


Fold 3
AUC: 0.8763
Classification Report:
              precision    recall  f1-score   support

           0       0.83      0.98      0.90     11157
           1       0.75      0.23      0.35      2837

    accuracy                           0.83     13994
   macro avg       0.79      0.61      0.63     13994
weighted avg       0.82      0.83      0.79     13994


Fold 4
AUC: 0.8661
Classification Report:
              precision    recall  f1-score   support

           0       0.83      0.98      0.90     11157
           1       0.76      0.23      0.35      2837

    accuracy                           0.83     13994
   macro avg       0.80      0.60      0.63     13994
weighted avg       0.82      0.83      0.79     13994


Fold 5
AUC: 0.8662
Classification Report:
              precision    recall  f1-score   support

           0       0.84      0.98      0.90     11157
           1       0.75      0.25      0.38      2837

    accuracy                           0.83     13994
   macro avg       0.79      0.62      0.64     13994
weighted avg       0.82      0.83      0.80     13994


--- Training GradientBoosting ---

Fold 1
AUC: 0.8786
Classification Report:
              precision    recall  f1-score   support

           0       0.86      0.95      0.91     11158
           1       0.69      0.41      0.51      2836

    accuracy                           0.84     13994
   macro avg       0.78      0.68      0.71     13994
weighted avg       0.83      0.84      0.83     13994


Fold 2
AUC: 0.8830
Classification Report:
              precision    recall  f1-score   support

           0       0.87      0.95      0.91     11158
           1       0.70      0.43      0.53      2836

    accuracy                           0.85     13994
   macro avg       0.79      0.69      0.72     13994
weighted avg       0.83      0.85      0.83     13994


Fold 3
AUC: 0.8856
Classification Report:
              precision    recall  f1-score   support

           0       0.87      0.95      0.91     11157
           1       0.70      0.44      0.54      2837

    accuracy                           0.85     13994
   macro avg       0.79      0.70      0.73     13994
weighted avg       0.84      0.85      0.83     13994


Fold 4
AUC: 0.8766
Classification Report:
              precision    recall  f1-score   support

           0       0.87      0.95      0.91     11157
           1       0.68      0.42      0.52      2837

    accuracy                           0.84     13994
   macro avg       0.77      0.68      0.71     13994
weighted avg       0.83      0.84      0.83     13994


Fold 5
AUC: 0.8766
Classification Report:
              precision    recall  f1-score   support

           0       0.87      0.95      0.91     11157
           1       0.68      0.43      0.53      2837

    accuracy                           0.84     13994
   macro avg       0.77      0.69      0.72     13994
weighted avg       0.83      0.84      0.83     13994


--- Training XGBoost ---

Fold 1
/Users/percival/anaconda3/envs/airbnb101/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [18:38:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
AUC: 0.9012
Classification Report:
              precision    recall  f1-score   support

           0       0.89      0.94      0.91     11158
           1       0.70      0.55      0.62      2836

    accuracy                           0.86     13994
   macro avg       0.79      0.75      0.77     13994
weighted avg       0.85      0.86      0.85     13994


Fold 2
/Users/percival/anaconda3/envs/airbnb101/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [18:38:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
AUC: 0.9030
Classification Report:
              precision    recall  f1-score   support

           0       0.89      0.94      0.92     11158
           1       0.69      0.56      0.62      2836

    accuracy                           0.86     13994
   macro avg       0.79      0.75      0.77     13994
weighted avg       0.85      0.86      0.86     13994


Fold 3
/Users/percival/anaconda3/envs/airbnb101/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [18:38:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
AUC: 0.9032
Classification Report:
              precision    recall  f1-score   support

           0       0.90      0.94      0.92     11157
           1       0.70      0.57      0.63      2837

    accuracy                           0.86     13994
   macro avg       0.80      0.76      0.77     13994
weighted avg       0.86      0.86      0.86     13994


Fold 4
/Users/percival/anaconda3/envs/airbnb101/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [18:38:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
AUC: 0.8986
Classification Report:
              precision    recall  f1-score   support

           0       0.89      0.94      0.91     11157
           1       0.69      0.55      0.61      2837

    accuracy                           0.86     13994
   macro avg       0.79      0.74      0.76     13994
weighted avg       0.85      0.86      0.85     13994


Fold 5
/Users/percival/anaconda3/envs/airbnb101/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [18:38:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
AUC: 0.8974
Classification Report:
              precision    recall  f1-score   support

           0       0.89      0.94      0.91     11157
           1       0.69      0.55      0.61      2837

    accuracy                           0.86     13994
   macro avg       0.79      0.75      0.76     13994
weighted avg       0.85      0.86      0.85     13994


--- Training LightGBM ---

Fold 1
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 11347, number of negative: 44629
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004181 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2623
[LightGBM] [Info] Number of data points in the train set: 55976, number of used features: 75
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.202712 -> initscore=-1.369430
[LightGBM] [Info] Start training from score -1.369430
AUC: 0.8943
Classification Report:
              precision    recall  f1-score   support

           0       0.88      0.94      0.91     11158
           1       0.69      0.51      0.59      2836

    accuracy                           0.85     13994
   macro avg       0.79      0.72      0.75     13994
weighted avg       0.84      0.85      0.85     13994


Fold 2
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 11347, number of negative: 44629
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003995 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2620
[LightGBM] [Info] Number of data points in the train set: 55976, number of used features: 75
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.202712 -> initscore=-1.369430
[LightGBM] [Info] Start training from score -1.369430
AUC: 0.9009
Classification Report:
              precision    recall  f1-score   support

           0       0.89      0.95      0.92     11158
           1       0.71      0.52      0.60      2836

    accuracy                           0.86     13994
   macro avg       0.80      0.73      0.76     13994
weighted avg       0.85      0.86      0.85     13994


Fold 3
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 11346, number of negative: 44630
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007796 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2613
[LightGBM] [Info] Number of data points in the train set: 55976, number of used features: 75
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.202694 -> initscore=-1.369541
[LightGBM] [Info] Start training from score -1.369541
AUC: 0.9012
Classification Report:
              precision    recall  f1-score   support

           0       0.89      0.94      0.91     11157
           1       0.70      0.53      0.60      2837

    accuracy                           0.86     13994
   macro avg       0.80      0.74      0.76     13994
weighted avg       0.85      0.86      0.85     13994


Fold 4
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 11346, number of negative: 44630
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004976 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2611
[LightGBM] [Info] Number of data points in the train set: 55976, number of used features: 75
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.202694 -> initscore=-1.369541
[LightGBM] [Info] Start training from score -1.369541
AUC: 0.8927
Classification Report:
              precision    recall  f1-score   support

           0       0.88      0.94      0.91     11157
           1       0.69      0.51      0.59      2837

    accuracy                           0.85     13994
   macro avg       0.79      0.73      0.75     13994
weighted avg       0.84      0.85      0.85     13994


Fold 5
[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
[LightGBM] [Info] Number of positive: 11346, number of negative: 44630
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004229 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2615
[LightGBM] [Info] Number of data points in the train set: 55976, number of used features: 75
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.202694 -> initscore=-1.369541
[LightGBM] [Info] Start training from score -1.369541
AUC: 0.8924
Classification Report:
              precision    recall  f1-score   support

           0       0.88      0.94      0.91     11157
           1       0.70      0.52      0.59      2837

    accuracy                           0.86     13994
   macro avg       0.79      0.73      0.75     13994
weighted avg       0.85      0.86      0.85     13994
