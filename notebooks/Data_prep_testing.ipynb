{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e336ff-0d2d-473b-a356-c196fdaac5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import Counter\n",
    "import math\n",
    "import matplotlib.ticker as mtick\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17771c07-11da-4bef-9203-468dd1a95fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set options to display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Set options to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e5056e-99f1-4b61-bff4-26f64bc4d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "raw_test_x = pd.read_csv(\"../data/raw/airbnb_test_x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba52091-9788-4c36-973a-e7d1477c62b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 61)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d259ecf-e6ea-4d97-8745-4ff45656091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = [\"access\",\n",
    "             \"host_about\",\n",
    "             \"host_acceptance_rate\",\n",
    "             \"interaction\",\n",
    "             \"jurisdiction_names\",\n",
    "             \"license\",\n",
    "             \"monthly_price\",\n",
    "             \"neighborhood_group\",\n",
    "             \"neighborhood_overview\",\n",
    "             \"notes\",\n",
    "             \"security_deposit\",\n",
    "             \"square_feet\",\n",
    "             \"weekly_price\",\n",
    "             'name',\n",
    "             'summary',\n",
    "             'space',\n",
    "             'description',\n",
    "             'experiences_offered',\n",
    "             'host_name',\n",
    "             'host_location', \n",
    "             'host_neighbourhood', \n",
    "             'street', \n",
    "             'neighborhood',\n",
    "             'state',\n",
    "             'zipcode',\n",
    "             'market',\n",
    "             'smart_location',\n",
    "             'country_code',\n",
    "             'house_rules',\n",
    "             'transit',\n",
    "             'country'\n",
    "            ]\n",
    "raw_test_x = raw_test_x.drop(columns=drop_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8baf1c5-97d1-434a-adf0-9b3a432ebf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets divide the data based on categorical and numerical features\n",
    "\n",
    "num_features = [feature for feature in raw_test_x.columns if raw_test_x[feature].dtype in ['float64', 'int64']] \n",
    "cat_features = [feature for feature in raw_test_x.columns if feature not in num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "342152f1-8e31-4401-97f6-d82487aab14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. lets deal with date features - host_since and first_review\n",
    "\n",
    "# Convert date strings to datetime objects\n",
    "raw_test_x['host_since'] = pd.to_datetime(raw_test_x['host_since'], errors='coerce')\n",
    "raw_test_x['first_review'] = pd.to_datetime(raw_test_x['first_review'], errors='coerce')\n",
    "\n",
    "# Get today's date\n",
    "today = pd.to_datetime(datetime.today().date())\n",
    "\n",
    "# Create new numerical features\n",
    "raw_test_x['Days_since_host_joined'] = (today - raw_test_x['host_since']).dt.days\n",
    "raw_test_x['Days_since_first_review'] = (today - raw_test_x['first_review']).dt.days\n",
    "\n",
    "#drop the old features\n",
    "raw_test_x = raw_test_x.drop(columns=['host_since','first_review'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19b05e19-ff7d-4796-bcb8-8736c8b85173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. amenities\n",
    "# Step 1: Clean and split the amenities strings into lists\n",
    "raw_test_x['amenities_cleaned'] = raw_test_x['amenities'].fillna('').apply(lambda x: [a.strip() for a in x.split(',') if a.strip()])\n",
    "\n",
    "# Step 2: Identify top 20 most frequent amenities\n",
    "all_amenities = [amenity for sublist in raw_test_x['amenities_cleaned'] for amenity in sublist]\n",
    "top_20_amenities = set([amenity for amenity, _ in Counter(all_amenities).most_common(20)])\n",
    "\n",
    "# Step 3: Filter each listing's amenities to only include top 20\n",
    "raw_test_x['amenities_top_20'] = raw_test_x['amenities_cleaned'].apply(lambda x: [a for a in x if a in top_20_amenities])\n",
    "\n",
    "# Step 4: One-hot encode using only top 20\n",
    "mlb = MultiLabelBinarizer()\n",
    "amenities_encoded = pd.DataFrame(mlb.fit_transform(raw_test_x['amenities_top_20']),\n",
    "                                 columns=[f'amenity_{a}' for a in mlb.classes_],\n",
    "                                 index=raw_test_x.index)\n",
    "\n",
    "# Step 5: Merge into final dataframe and drop unused columns\n",
    "raw_test_x = pd.concat([raw_test_x.drop(columns=['amenities', 'amenities_cleaned', 'amenities_top_20']), amenities_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec688e53-1424-4a8c-b2f4-0e925042b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. host_verifications\n",
    "# Clean and split the string into list\n",
    "raw_test_x['host_verifications_cleaned'] = raw_test_x['host_verifications'].fillna('').apply(\n",
    "    lambda x: [v.strip() for v in x.split(',') if v.strip()]\n",
    ")\n",
    "\n",
    "# Flatten the list and count frequency\n",
    "all_verifications = [v for sublist in raw_test_x['host_verifications_cleaned'] for v in sublist]\n",
    "top_n = 10\n",
    "top_verifications = Counter(all_verifications).most_common(top_n)\n",
    "\n",
    "# Get just the top N verification methods\n",
    "top_verif_set = set([v for v, _ in top_verifications])\n",
    "\n",
    "# Keep only top N in each row\n",
    "raw_test_x['host_verifications_top'] = raw_test_x['host_verifications_cleaned'].apply(\n",
    "    lambda x: [v for v in x if v in top_verif_set]\n",
    ")\n",
    "\n",
    "# One-hot encode\n",
    "mlb = MultiLabelBinarizer()\n",
    "verif_encoded = pd.DataFrame(\n",
    "    mlb.fit_transform(raw_test_x['host_verifications_top']),\n",
    "    columns=[f'verif_{v}' for v in mlb.classes_],\n",
    "    index=raw_test_x.index\n",
    ")\n",
    "\n",
    "# Merge with main DataFrame\n",
    "raw_test_x = pd.concat(\n",
    "    [raw_test_x.drop(columns=['host_verifications', 'host_verifications_cleaned', 'host_verifications_top']),\n",
    "     verif_encoded],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a3ecf9d-d036-4082-b957-bd93a353b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. features\n",
    "# Clean and split\n",
    "raw_test_x['features_cleaned'] = raw_test_x['features'].fillna('').apply(\n",
    "    lambda x: [f.strip() for f in x.split(',') if f.strip()]\n",
    ")\n",
    "\n",
    "# Flatten and count\n",
    "all_features = [f for sublist in raw_test_x['features_cleaned'] for f in sublist]\n",
    "top_n = 8\n",
    "top_features = Counter(all_features).most_common(top_n)\n",
    "\n",
    "# Get top N as a set\n",
    "top_feature_set = set([f for f, _ in top_features])\n",
    "\n",
    "# Filter each row to retain only top N\n",
    "raw_test_x['features_top'] = raw_test_x['features_cleaned'].apply(\n",
    "    lambda x: [f for f in x if f in top_feature_set]\n",
    ")\n",
    "\n",
    "# One-hot encode\n",
    "mlb = MultiLabelBinarizer()\n",
    "features_encoded = pd.DataFrame(\n",
    "    mlb.fit_transform(raw_test_x['features_top']),\n",
    "    columns=[f'feature_{f}' for f in mlb.classes_],\n",
    "    index=raw_test_x.index\n",
    ")\n",
    "\n",
    "# Final merge and cleanup\n",
    "raw_test_x = pd.concat(\n",
    "    [raw_test_x.drop(columns=['features', 'features_cleaned', 'features_top']),\n",
    "     features_encoded],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c977368-b376-4505-babc-9283186e20ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 65)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1092d5fe-c35b-42e7-a213-398c3e3df156",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test_x_2 = raw_test_x.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d99a423-cb4a-46b8-8acf-5add6d81ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing spaces from column names\n",
    "raw_test_x_2.columns = raw_test_x_2.columns.str.replace(' ', '_').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f983366-a438-420e-a603-50f20859e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = [feature for feature in raw_test_x_2.columns if raw_test_x_2[feature].dtype in ['float64', 'int64']] \n",
    "cat_features = [feature for feature in raw_test_x_2.columns if feature not in num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb372d14-c38c-493f-9261-146770bc7954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. host_response_time\n",
    "# Define ordinal mapping\n",
    "response_time_mapping = {\n",
    "    \"within an hour\": 3,\n",
    "    \"within a few hours\": 2,\n",
    "    \"within a day\": 1,\n",
    "    \"a few days or more\": 0\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "raw_test_x_2['host_response_time_encoded'] = raw_test_x_2['host_response_time'].map(response_time_mapping)\n",
    "\n",
    "raw_test_x_2.drop(columns=['host_response_time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43c37360-e99a-4b2d-9f47-8e81c512fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. bed_type\n",
    "raw_test_x_2['bed_type_encoded'] = raw_test_x_2['bed_type'].apply(lambda x: 1 if x == 'Real Bed' else 0)\n",
    "raw_test_x_2.drop(columns=['bed_type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7948104-0460-4baa-8fe6-017afaa5c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. property_type\n",
    "\n",
    "property_type_mapping = {\n",
    "    'Other': 0,\n",
    "    'Condominium': 1,\n",
    "    'Townhouse': 2,\n",
    "    'Loft': 3,\n",
    "    'House': 4,\n",
    "    'Apartment': 5\n",
    "}\n",
    "\n",
    "raw_test_x_2['property_type_encoded'] = raw_test_x_2['property_type'].apply(\n",
    "    lambda x: x if x in property_type_mapping else 'Other'\n",
    ")\n",
    "\n",
    "raw_test_x_2['property_type_encoded'] = raw_test_x_2['property_type_encoded'].map(property_type_mapping)\n",
    "\n",
    "\n",
    "raw_test_x_2.drop(columns=['property_type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8785d4f-bd0b-45d8-bdd5-40fe128c70b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. room_type\n",
    "\n",
    "# One-hot encode with drop_first=True\n",
    "room_type_dummies = pd.get_dummies(raw_test_x_2['room_type'], prefix='room_type', drop_first=True)\n",
    "\n",
    "# Convert boolean to int (True → 1, False → 0)\n",
    "room_type_dummies = room_type_dummies.astype(int)\n",
    "\n",
    "# Add to dataframe and drop original\n",
    "raw_test_x_2 = pd.concat([raw_test_x_2, room_type_dummies], axis=1)\n",
    "raw_test_x_2.drop(columns=['room_type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17c7d008-2f10-43b5-9154-1c4a81a8ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing spaces from column names\n",
    "raw_test_x_2.columns = raw_test_x_2.columns.str.replace(' ', '_').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9eb8e9dd-4315-4429-b2dd-bda53dc845bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 9. cancellation_policy\n",
    "\n",
    "# Define rare policies\n",
    "rare_policies = ['no_refunds', 'super_strict_30', 'super_strict_60']\n",
    "\n",
    "# Replace rare with 'other'\n",
    "raw_test_x_2['cancellation_policy_clean'] = raw_test_x_2['cancellation_policy'].apply(\n",
    "    lambda x: 'other' if x in rare_policies else x\n",
    ")\n",
    "\n",
    "# Apply manual ordinal encoding\n",
    "cancellation_policy_mapping = {\n",
    "    'other': 0,\n",
    "    'flexible': 1,\n",
    "    'moderate': 2,\n",
    "    'strict': 3\n",
    "}\n",
    "\n",
    "raw_test_x_2['cancellation_policy_encoded'] = raw_test_x_2['cancellation_policy_clean'].map(cancellation_policy_mapping)\n",
    "\n",
    "# Optional: drop intermediate/old columns\n",
    "raw_test_x_2.drop(columns=['cancellation_policy', 'cancellation_policy_clean'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09594109-2589-4a95-ac85-78ad40f051c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. city\n",
    "\n",
    "# Fixing extreme skew + noise (like \"brooklyn\", \"Brooklyn\", \"BROOKLYN\")\n",
    "raw_test_x_2['city_clean'] = raw_test_x_2['city'].str.strip().str.lower()\n",
    "\n",
    "# Step 2: Get top 10 cities by frequency\n",
    "top_10_cities = raw_test_x_2['city_clean'].value_counts().head(10).index.tolist()\n",
    "\n",
    "# Step 3: Group all other cities under 'other'\n",
    "raw_test_x_2['city_grouped'] = raw_test_x_2['city_clean'].apply(lambda x: x if x in top_10_cities else 'other')\n",
    "\n",
    "# Step 4: One-hot encode (drop_first to avoid dummy trap) and convert to 0/1\n",
    "city_dummies = pd.get_dummies(raw_test_x_2['city_grouped'], prefix='city', drop_first=True).astype(int)\n",
    "\n",
    "# Step 5: Combine and clean up\n",
    "raw_test_x_2 = pd.concat([raw_test_x_2, city_dummies], axis=1)\n",
    "\n",
    "raw_test_x_2.drop(columns=['city', 'city_clean', 'city_grouped'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9df2e095-c9ea-472c-8d3c-0f45a5f2b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load your processed train and test data\n",
    "train_df = pd.read_csv(\"../data/processed/train_exp_ready_no_feature_scaling.csv\")\n",
    "\n",
    "# Step 2: Identify all city-related dummy columns from the train set\n",
    "city_columns = [col for col in train_df.columns if col.startswith('city_')]\n",
    "\n",
    "# Step 3: Add missing city columns to test_df and fill with 0\n",
    "for col in city_columns:\n",
    "    if col not in raw_test_x_2.columns:\n",
    "        raw_test_x_2[col] = 0\n",
    "\n",
    "# Step 4: Drop extra city columns in test_df (not in training)\n",
    "for col in raw_test_x_2.columns:\n",
    "    if col.startswith('city_') and col not in city_columns:\n",
    "        raw_test_x_2.drop(columns=[col], inplace=True)\n",
    "\n",
    "# Step 5: Optional — reorder columns to match train_df (if needed)\n",
    "# This is helpful if you want to ensure alignment before predict\n",
    "columns_order = [col for col in train_df.columns if col != 'high_booking_rate']\n",
    "raw_test_x_2 = raw_test_x_2[columns_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e61042b2-625b-4cf9-b377-a41f20679e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test_x = raw_test_x_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfefa436-6edc-440b-b255-bc1a9564190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output this data into a csv as train_data_proccessed_1\n",
    "processed_test_x.to_csv('../data/processed/processed_test_x.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c1493-4b21-4980-b764-cbc73d87ded8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
